{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# imports all dependencies\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas._libs.tslibs.timestamps import Timestamp\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "4nYtXEn6DL5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data initialisation definitions for uses are 0=raw, 1=autoregressive, ...\n",
        "\n",
        "class data_functions:\n",
        "\n",
        "  def __init__(self, file_path, use):\n",
        "    self.file_path =  file_path\n",
        "    self.use = use\n",
        "\n",
        "    # imports data in the format of a csv with index, Date, Average_flux for graphing raw data\n",
        "    self.df = pd.read_csv(self.file_path)\n",
        "\n",
        "    # sets date to datetime format and as index value\n",
        "    self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
        "    self.df.set_index('Date', inplace=True)\n",
        "\n",
        "    # for use 1=autoregressive model, sets back log (n=(1-back_log)) and turns raw into log flux\n",
        "    if self.use == 1:\n",
        "      back_log = 11\n",
        "      for i in range(1, back_log):\n",
        "        self.df[f'Average_Flux_{i}_days_ago'] = self.df['Average_Flux'].shift(i)\n",
        "      # turns values from raw data to logarithm\n",
        "      def transform_flux(value):\n",
        "        return np.log10(abs(value) + 1e-10)\n",
        "      columns_to_transform = ['Average_Flux'] + [f'Average_Flux_{i}_days_ago' for i in range(1, back_log)]\n",
        "      self.df[columns_to_transform] = self.df[columns_to_transform].applymap(transform_flux)\n",
        "      # adds appropriate label changes to column headers\n",
        "      self.df = self.df.add_prefix('log_')\n",
        "      columns_to_transform2 = ['log_Average_Flux'] + [f'log_Average_Flux_{i}_days_ago' for i in range(1, back_log)]\n",
        "      #  optional absolute values\n",
        "      #self.df[columns_to_transform2] = self.df[columns_to_transform2].applymap(abs)\n",
        "\n",
        "\n",
        "    # drops N/A value rows\n",
        "    self.df.dropna(inplace=True)\n",
        "    # output as csv for self.df with format date (as index) and log average flux values for n days previous plus current day\n",
        "\n",
        "  def graph(self):\n",
        "    #flux log average graph\n",
        "    if self.use == 1:\n",
        "      self.df['log_Average_Flux'].plot(kind='line', figsize=(16, 8), title='log_Average_Flux', color='black')\n",
        "      self.max_value = self.df['log_Average_Flux'].max()\n",
        "      self.min_value = self.df['log_Average_Flux'].min()\n",
        "      print(self.max_value, self.min_value)\n",
        "    #flux raw data graph\n",
        "    if  self.use == 0:\n",
        "      self.df['Average_Flux'].plot(kind='line', figsize=(16, 8), title='Average_Flux', color='black')\n",
        "      self.max_value = self.df['Average_Flux'].max()\n",
        "      self.min_value = self.df['Average_Flux'].min()\n",
        "      print(self.max_value, self.min_value)\n",
        "\n",
        "  # Split the data into features and target, takes full csv as X bar the log_Average_Flux which is set to y\n",
        "  def split(self):\n",
        "    if self.use == 0:\n",
        "      return 0, 0\n",
        "    else:\n",
        "      self.X = self.df.drop('log_Average_Flux', axis=1)\n",
        "      self.y = self.df['log_Average_Flux']\n",
        "      return self.X, self.y"
      ],
      "metadata": {
        "id": "Ua343UDthLtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model, initialise sets parameters and trains the model\n",
        "class NN_model:\n",
        "  def __init__(self, opt, learn_rate, dataframe, X, y):\n",
        "\n",
        "    #sets scaler\n",
        "    self.scaler = MinMaxScaler()\n",
        "    self.dataframe = dataframe\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "    # scales the data between 0 and 1\n",
        "    for col in self.dataframe.columns:\n",
        "      column_data = self.dataframe[col].values.reshape(-1, 1)\n",
        "      # reshapes 1D array of columns in dataframe to 2D array with 1 column\n",
        "      self.dataframe[col] = self.scaler.fit_transform(column_data)\n",
        "\n",
        "    #sets random seed\n",
        "    random_seed = 30\n",
        "    tf.random.set_seed(random_seed)\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create a neural network model\n",
        "    self.model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=self.X_train.shape[1]),\n",
        "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=None)\n",
        "    ])\n",
        "\n",
        "    #learning rate selection\n",
        "    self.learning_rate = learn_rate\n",
        "\n",
        "    # Optimizer selection\n",
        "    if opt == 1:\n",
        "      self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "    # stochastic gradient descent\n",
        "    else:\n",
        "      self.optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
        "\n",
        "    # Compile the model\n",
        "    self.model.compile(loss=tf.keras.losses.mae, optimizer=self.optimizer, metrics=[\"mae\"])\n",
        "\n",
        "  def train(self):\n",
        "    # Train the model\n",
        "    self.history = self.model.fit(self.X_train, self.y_train, epochs=400, batch_size=150, validation_data=(self.X_test, self.y_test), shuffle=True)\n",
        "    # Evaluate the model\n",
        "    self.loss = self.model.evaluate(self.X_test, self.y_test)\n",
        "    plt.plot(self.history.history['loss'], label='Training Loss')\n",
        "    plt.plot(self.history.history['val_loss'], label='Validation Loss', linestyle='--')\n",
        "    plt.title('Loss Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  def predict(self):\n",
        "    # Make predictions on the test set\n",
        "    self.y_pred = self.model.predict(self.X_test)\n",
        "    # y_pred is an array in the form of non-scaled logarithm value ~(5<y_pred>-1)\n",
        "\n",
        "  def graph(self):\n",
        "    # graphs predicted and actaul log average flux values against one another\n",
        "    self.df_graph  = pd.DataFrame(self.y_test)\n",
        "    self.df_graph['y_prediction'] = self.y_pred\n",
        "    self.df_graph.sort_values(by='Date', inplace = True)\n",
        "\n",
        "    # date range specification if wanting the full graph just set to really old date for start_date and future date for end_date\n",
        "    start_date = Timestamp(datetime(1900, 1, 1), tz='UTC')\n",
        "    end_date = Timestamp(datetime(2020, 12, 31), tz='UTC')\n",
        "    if start_date <= self.df_graph.index[0]:\n",
        "      start_date_str = self.df_graph.index[0].strftime('%Y-%m-%d')\n",
        "    else:\n",
        "      start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "    if end_date >= self.df_graph.index[-1]:\n",
        "      end_date_str = self.df_graph.index[-1].strftime('%Y-%m-%d')\n",
        "    else:\n",
        "      end_date_str = end_date.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Filter the DataFrame based on the date range\n",
        "    df_subset = self.df_graph[(self.df_graph.index >= start_date) & (self.df_graph.index <= end_date)]\n",
        "\n",
        "    #plot\n",
        "    plt.figure(figsize=(20, 12))\n",
        "    plt.plot(df_subset.index, df_subset['log_Average_Flux'], label='Real Flux', color='black')\n",
        "    plt.plot(df_subset.index, df_subset['y_prediction'], label='Flux prediction', linestyle='--', color='red', alpha=0.8)\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Log Average Flux')\n",
        "    plt.title('Log Average Flux and Flux Prediction between: ' + start_date_str + ' and ' + end_date_str)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  def scatter_graph(self):\n",
        "    # scatter plot currently of no use in format\n",
        "    plt.scatter(self.y_test, self.y_pred, color='green', marker='x')\n",
        "    plt.xlabel('Log Actual Values')\n",
        "    plt.ylabel('Log Predicted Values')\n",
        "    plt.title('Log Average Actual vs Predicted Values')\n",
        "    plt.show()\n",
        "\n",
        "  def validate(self):\n",
        "    #validation with PE of the model\n",
        "    mse = mean_squared_error(self.df_graph['log_Average_Flux'], self.df_graph['y_prediction'])\n",
        "    variance = self.df_graph['log_Average_Flux'].var()\n",
        "    PE = 1-(mse/variance)\n",
        "    print(PE)\n",
        "\n",
        "  def save(self, results_file_name, model_file_name):\n",
        "    # saves the dataframe of result, does not save the model\n",
        "    final_file_path = '/content/drive/MyDrive/4th year/diss/models and results/' + results_file_name + '.csv'\n",
        "    self.df_graph.to_csv(final_file_path, index=True)\n",
        "    folder_path = '/content/drive/MyDrive/4th year/diss/models and results/saved models/' + model_file_name\n",
        "    !mkdir \"$folder_path\"\n",
        "    self.model.save(folder_path)\n",
        "    #print(final_file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "UHDcA8qSDwL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_functions('/content/drive/MyDrive/4th year/diss/Data_for_FYP/Energetic particle sensor/GOES  9, 10, 11 /full/G9_10_11_1995-2010.csv', 1)\n",
        "df_ave  = data.df\n",
        "X, y = data_functions.split(data)\n",
        "#data_functions.graph(data)\n",
        "m = NN_model(1, 0.0001, df_ave, X, y)\n",
        "NN_model.train(m)\n",
        "NN_model.predict(m)"
      ],
      "metadata": {
        "id": "H9hnc_mdrPXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model.graph(m)\n",
        "NN_model.save(m, 'results 1', 'model 1')"
      ],
      "metadata": {
        "id": "oz4hq04M3Uoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model.scatter_graph(m)"
      ],
      "metadata": {
        "id": "KyKKOdOSMGUA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}